apiVersion: work.open-cluster-management.io/v1alpha1
kind: ManifestWorkReplicaSet
metadata:
  name: my-dataflow
  namespace: default
spec:
  manifestWorkTemplate:
    manifestConfigs:
      - feedbackRules:
          - jsonPaths:
              - name: phase
                path: .status.phase
            type: JSONPaths
        resourceIdentifier:
          group: data.fluid.io
          name: model-dataload
          namespace: default
          resource: dataloads
    workload:
      manifests:
      - apiVersion: data.fluid.io/v1alpha1
        kind: DataLoad
        metadata:
          name: model-dataload
          namespace: default
        spec:
          dataset:
            name: model-data
            namespace: default
          loadMetadata: true
          target:
          - path: /models
            replicas: 1
      - apiVersion: data.fluid.io/v1alpha1
        kind: DataProcess
        metadata:
          name: apply-my-gpu-app
          namespace: default
        spec:
          dataset:
            name: model-data
            namespace: default
            mountPath: ""
          # DataProcess job will run after `model-dataload` job finishes.
          runAfter:
            kind: DataLoad
            name: model-dataload
            namespace: default
          processor:
            serviceAccountName: apply-my-gpu-app
            script:
              # image: registry.cn-beijing.aliyuncs.com/fluid-namespace/docker-hub-mirror
              # imageTag: bitnami-kubectl-1.28.12
              image: bitnami/kubectl
              imageTag: 1.28.12
              command:
              - kubectl
              - apply
              - -f
              source: |
                apiVersion: apps/v1
                kind: Deployment
                metadata:
                  labels:
                    app: qwen
                  name: qwen-vllm
                  namespace: default
                spec:
                  replicas: 1
                  selector:
                    matchLabels:
                      app: qwen
                  strategy:
                    # rollingUpdate:
                    #   maxSurge: 25%
                    #   maxUnavailable: 25%
                    # type: RollingUpdate
                    type: Recreate
                  template:
                    metadata:
                      labels:
                        app: qwen
                    spec:
                      containers:
                      - args:
                        - -c
                        - exec python3 -m vllm.entrypoints.openai.api_server --trust-remote-code
                          --model /model/models/ --gpu-memory-utilization 0.95 --max-model-len 8192
                          --dtype half
                          --enforce-eager
                          # ValueError: The model's max seq len (16384) is larger than the maximum number of tokens that can be stored in KV cache (9584). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
                        command:
                        - /bin/bash
                        # image: quay.io/zhujian/kube-ai-vllm:0.4.1
                        # image: registry.cn-beijing.aliyuncs.com/fluid-namespace/vllm:0.4.1-nsys-optim
                        image: quay.io/zhujian/kube-ai-vllm:0.4.1-nsys-optim
                        name: qwen
                        env:
                        # - name: PYTORCH_CUDA_ALLOC_CONF
                        #   value: "expandable_segments:True"
                        ports:
                        - containerPort: 8000
                          name: qwen
                        readinessProbe:
                          httpGet:
                            path: /v1/models
                            port: 8000
                          initialDelaySeconds: 120   # Wait 5 minutes before starting readiness checks
                          periodSeconds: 10          # Check readiness every 10 seconds
                          timeoutSeconds: 5          # Timeout for each readiness check
                          failureThreshold: 5        # Number of consecutive failures before marking the pod as Unready
                        livenessProbe:
                          httpGet:
                            path: /v1/models
                            port: 8000
                          initialDelaySeconds: 120   # Wait 5 minutes before starting liveness checks
                          periodSeconds: 20          # Check liveness every 20 seconds
                          timeoutSeconds: 5          # Timeout for each liveness check
                          failureThreshold: 5        # Number of consecutive failures before restarting the container
                        resources:
                          limits:
                            nvidia.com/gpu: "1"
                            memory: 16Gi
                          requests:
                            nvidia.com/gpu: "1"
                            memory: 2Gi
                        volumeMounts:
                        - mountPath: /dev/shm
                          name: shm
                        - mountPath: /model
                          name: models
                      restartPolicy: Always
                      volumes:
                      - hostPath:
                          path: /dev/shm
                        name: shm
                      - name: models
                        persistentVolumeClaim:
                          claimName: model-data
                      tolerations:
                      - key: "nvidia.com/gpu"
                        operator: "Exists"
                        effect: "NoSchedule"
      - apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: apply-my-gpu-app
          namespace: default
      - kind: Role
        apiVersion: rbac.authorization.k8s.io/v1
        metadata:
          name: apply-my-gpu-app-role
          namespace: default
        rules:
          - apiGroups: ["apps"]
            resources: 
              - deployments
            verbs:
              - get
              - create
              - patch
              - update
      - apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: apply-my-gpu-app-rolebinding
          namespace: default
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: apply-my-gpu-app-role
        subjects:
          - kind: ServiceAccount
            name: apply-my-gpu-app
            namespace: default
  placementRefs:
  - name: ${PLACEMENT_NAME}
    rolloutStrategy:
      type: All
